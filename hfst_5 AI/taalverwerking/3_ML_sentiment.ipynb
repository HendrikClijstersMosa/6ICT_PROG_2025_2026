{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#FFFFFF markdown=\"1\">\n",
    "        <h1> <center> Gebaseerd op een cursus van:</center> </h1> \n",
    "    </font>\n",
    "    <a href=\"https://www.aiopschool.be/chatbot/\"> \n",
    "        <img src=\"../_afbeeldingen/bannerugentdwengo.png\" alt=\"Dwengo\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 30px; width:20%\"/>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B0000\"> Activeer rechtsboven de venv 'venv_AI' als kernel van deze Notebook! </div>\n",
    "\n",
    "<br>\n",
    "<img src=\"../_afbeeldingen/kernel_venvAI.png\" alt=\"Banner\" style =\"display: block; margin-left: auto; margin-right: auto; width: 40%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toepassing: ML (lerende) Sentimentanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B8000\">\n",
    "Voor deze notebook te starten. Zorg zeker dat je de inhoud van 2_AI_sentiment.ipynb begrijpt. In deze notebook zal je dezelfde concepten gebruiken. Nu zal echter een ML-model gebruikt worden om de vervelende stappen uit 2_AI_sentiment.ipynb te automatiseren.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algemene info bij Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Modules installeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze notebook zal je de module <a href=\"https://spacy.io/\">**spacy**</a> gebruiken. Deze module bevat een Machine Learning taalmodel. We zullen deze gebruiken om bepaalde stappen van het voorverwerken te vereenvoudigen (zie ook *deel 1.3 'Doel ML Sentimentanalyse'*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B0000\"> \n",
    "Wees zeker dat je rechtsboven in de juiste kernel zit vooraleer spacy te installeren.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.24.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "     ---------------------------------------- 0.0/90.6 kB ? eta -:--:--\n",
      "     ------------------------------------ --- 81.9/90.6 kB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 90.6/90.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting typer>=0.24.0 (from typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading typer-0.24.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.1.1-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting click>=8.2.1 (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=12.3.0 (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hendrik.clijsters\\desktop\\_venv\\venv_ai\\lib\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.2 MB 6.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/14.2 MB 9.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.9/14.2 MB 7.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.5/14.2 MB 8.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.0/14.2 MB 8.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.1/14.2 MB 7.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.0/14.2 MB 9.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.5/14.2 MB 9.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.9/14.2 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.4/14.2 MB 9.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.0/14.2 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.5/14.2 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.0/14.2 MB 10.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.3/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.8/14.2 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.1/14.2 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.4/14.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.9/14.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.2/14.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.7/14.2 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.3/14.2 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.5/14.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.1/14.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.6/14.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.0/14.2 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.5/14.2 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.0/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.5/14.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.1/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.4/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.0/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 8.7 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.2/40.2 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "   ---------------------------------------- 0.0/118.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 118.3/118.3 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "   ---------------------------------------- 0.0/463.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  460.8/463.6 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 463.6/463.6 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/2.0 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.0 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.8 kB ? eta -:--:--\n",
      "   -------------------- ------------------ 337.9/654.8 kB 20.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  645.1/654.8 kB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 654.8/654.8 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.4/1.7 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 12.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.7 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading typer_slim-0.24.0-py3-none-any.whl (3.4 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.2 MB 10.9 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.5/6.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.0/6.2 MB 5.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.4/6.2 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 6.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.4/6.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.8/6.2 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.6/6.2 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.1/6.2 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.2 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.6/6.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.1/6.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading smart_open-7.5.1-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.1/64.1 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.24.1-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "   ---------------------------------------- 0.0/108.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 108.3/108.3 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading rich-14.3.3-py3-none-any.whl (310 kB)\n",
      "   ---------------------------------------- 0.0/310.5 kB ? eta -:--:--\n",
      "   --------------------------------------  307.2/310.5 kB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 310.5/310.5 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading wrapt-2.1.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.3/60.3 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.3/87.3 kB 4.8 MB/s eta 0:00:00\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: wrapt, wasabi, typing-inspection, tqdm, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, cymem, cloudpathlib, click, catalogue, blis, annotated-types, annotated-doc, srsly, smart-open, pydantic, preshed, markdown-it-py, rich, confection, typer, thinc, typer-slim, weasel, spacy\n",
      "Successfully installed annotated-doc-0.0.4 annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 click-8.3.1 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 rich-14.3.3 shellingham-1.5.4 smart-open-7.5.1 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 tqdm-4.67.3 typer-0.24.1 typer-slim-0.24.0 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3 wrapt-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-3.8.0/nl_core_news_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 245.8 kB/s eta 0:00:53\n",
      "     --------------------------------------- 0.1/12.8 MB 714.4 kB/s eta 0:00:18\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.9/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.2/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.6/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.2/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.9/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.9/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.9/12.8 MB 7.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.7/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: nl-core-news-sm\n",
      "Successfully installed nl-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('nl_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installeren van spacy (dit kan even duren)\n",
    "!pip3 install spacy\n",
    "# installeren van Nederlandse bibliotheek\n",
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Herhaling AI Sentimentanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de vorige notebook, 'AI sentimentanalyse', maakte je kennis met de principes van een regelgebaseerde (AI) sentimentanalyse: \n",
    "\n",
    " -  Het maakt gebruik van een **lexicon** of woordenboek met daarin woorden gekoppeld aan hun **sentiment** (positief, negatief of neutraal).\n",
    " -  Voor een sentimentanalyse uit te voeren op een tekst, moet de tekst eerst **voorverwerkt** worden. Veelvoorkomende preprocessing stappen zijn **lowercasing**, **tokenisering**, **woordsoort tagging** en **lemmatisering**.\n",
    " - De sentimentanalyse overloopt de combinatie van woordsoort/lemma voor ieder woord in de tekst. Zit deze in het lexicon? Tel dan de sentimentscore op bij een teller.\n",
    " \n",
    "In de vorige notebook waren nog niet alle stappen geautomatiseerd. **Lemmatisering en woordsoort tagging** moesten manueel gebeuren. Zoals te zien is op onderstaand stappenplan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../_afbeeldingen/AI_sentiment.png\" alt=\"Banner\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px; width:80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Doel ML Sentimentanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze notebook zal je de uitvoer van de sentimentanalyse volledig <b>automatiseren</b>. Je zal m.a.w. de computer al het werk laten doen: de computer zal de data preprocessen (voorverwerken) met het <em>machine learning-model (ML-model) Spacy</em>. Vervolgens zal het via een <em>regelgebaseerd AI-systeem</em> de <b>lemmas/woordsoorten</b> matchen in het lexicon. Tenslotte neemt de AI op basis van de score een beslissing over het sentiment van de gegeven tekst. Onderstaand stappenplan toont het nieuwe proces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../_afbeeldingen/ML_sentiment.png\" alt=\"Banner\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px; width:80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing met Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modules & variabelen klaarzetten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaande code importeert spacy in deze Notebook. Vervolgens geven we aan welke taalpakket we in spacy willen laden. nl_core_news_sm is het kleinste taalpakket dat voor Nederlands beschikbaar is. Maar ondanks dit heeft het een accuraatheid van 93%!. Anderen zijn <a href=\"https://spacy.io/usage/models\">HIER</a> te vinden.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeren van spacy\n",
    "import spacy\n",
    "# inladen van Nederlandse bibliotheek\n",
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naast het model zetten we ook opnieuw de review klaar die we willen voorverwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Nieuw concept in Gent, maar dat kan volgens mij toch beter. De meeste cornflakes waren gewoon de basic soorten. Ook wat duur voor de hoeveelheid die je krijgt, vooral met de toppings zijn ze zuinig. En als je ontbijt aanbiedt, geef de mensen dan toch ook wat meer keuze voor hun koffie.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B8000\">\n",
    "Het lijkt misschien valspelen dat we andermans ML-model gebruiken. Maar onthoud dat je als programmeur vooral praktisch (lees lui) moet zijn. Het opstellen van een eigen ML-model voor tekstherkenning is mogelijk, maar zou ons veel tijd kosten. Je moet namelijk over heel veel gelabelde data bezitten. Bedenk dat je voor miljoenen woorden een lemma en woordsoort moet opstellen...\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lowercasing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B0000\"> \n",
    "Merk op dat deze stap identiek is aan 'deel 4.2 Lowercasing' van de vorige notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aller eerste stap is het omzetten van `review` naar kleine letters. Gelukkig is dit ingebouwd in Python! Zet de tekst in de variabele `review` om naar kleine letters (via welke methode?). Sla het resultaat op in de variabele `review_kleineletters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zet tekst van de review om naar tekst in kleine letters\n",
    "\n",
    "\n",
    "# toon resultaat van lowercasing (er mag geen drukletter meer in de review staan)\n",
    "print(review_kleineletters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenisering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervolgens moeten we alle woorden in de tekst splitsen en aan een lijst toevoegen.\n",
    "\n",
    "In voorgaande notebook is de **tokenisering** regelgebaseerd uitgevoerd. het **spacy**-model zal dit echter ook voor zijn rekening nemen (samen met **woordsoort tagging** en **lemmatisering**)! Voer onderstaand code-blok uit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review met kleine letters aan model voeren\n",
    "review_tokens = nlp(review_kleineletters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit was alles. In `review_tokens` kan je de tokens van de review vinden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B8000\"> \n",
    "Merk op dat de leestekens er nu wel bijstaan. Dit is geen probleem. Spacy zal deze ook een correcte woordsoort & lemma geven.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B0000\"> \n",
    "De print maakt deze notebook zeer lang. Duw bovenaan op de knop 'clear all outputs' om deze terug te verwijderen.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print ieder het token zelf\n",
    "for token in review_tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Woordsoort tagging en Lemmatisering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `review_tokens` zitten niet alleen de **tokens**, maar ook reeds de **woordsoorten** & **lemma's**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review met kleine letters aan model voeren\n",
    "review_tokens = nlp(review_kleineletters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print de woordsoort tag van elk token\n",
    "for token in review_tokens:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print de lemma van elk token\n",
    "for token in review_tokens:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B8000\"> \n",
    "Merk op dat <b>spacy</b> de woordsoort in het Engels geeft. Dit is ook de reden waarom het <b>Lexicon</b> Engelse afkortingen gebruikt.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oefen mee 2.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doorloop `review_tokens` en stel twee lijsten op `review_lemmas` en `review_soort`.\n",
    "- De lijst `review_lemmas` bevat alle lemma's (in correcte volgorde).\n",
    "- De lijst `review_soort` bevat de woordsoorten van alle tokens (in correcte volgorde).\n",
    "\n",
    "Print deze lijsten na het opstellen.\n",
    "\n",
    "Als voorbeeld:<br>\n",
    "- `review_voorverwerkt` = \"gent , maar dat kan\" <br>\n",
    "- `review_lemmas` = [\"Gent\", \",\", \"maar\", \"dat\", \"kunnen\"] <br>\n",
    "- `review_soort` = [\"PROPN\", \"PUNCT\", \"CCONJ\", \"PRON\", \"AUX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lemmas = []\n",
    "review_soort  = [] \n",
    "\n",
    "# TODO: overloop alle tokens en voeg lemma/woordsoort toe aan bovenstaande lijsten.\n",
    "\n",
    "# Print aangevuld lijsten\n",
    "print(review_lemmas)\n",
    "print(review_soort)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B0000\"> \n",
    "Ga pas verder als de printen volgend resultaat geven:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lemma = ['nieuw', 'concept', 'in', 'Gent', ',', 'maar', 'dat', 'kunnen', 'volgens', 'mij', 'toch', 'goed', '.', 'de', 'veel', 'cornflakes', 'zijn', 'gewoon', 'de', 'basic', 'soort', '.', 'ook', 'wat', 'duur', 'voor', 'de', 'hoeveelheid', 'die', 'je', 'krijgen', ',', 'vooral', 'met', 'de', 'toppings', 'zijn', 'ze', 'zuinig', '.', 'en', 'als', 'je', 'ontbijt', 'aanbieden', ',', 'geven', 'de', 'mens', 'dan', 'toch', 'ook', 'wat', 'veel', 'keuze', 'voor', 'hun', 'koffie', '.']\n",
    "review_soort = ['ADJ', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'CCONJ', 'PRON', 'AUX', 'ADP', 'PRON', 'ADV', 'ADJ', 'PUNCT', 'DET', 'ADV', 'NOUN', 'AUX', 'ADJ', 'DET', 'NOUN', 'NOUN', 'PUNCT', 'ADV', 'PRON', 'ADJ', 'ADP', 'DET', 'NOUN', 'PRON', 'PRON', 'VERB', 'PUNCT', 'ADV', 'ADP', 'DET', 'NOUN', 'AUX', 'PRON', 'ADJ', 'PUNCT', 'CCONJ', 'SCONJ', 'PRON', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'DET', 'NOUN', 'ADV', 'ADV', 'ADV', 'PRON', 'DET', 'NOUN', 'ADP', 'PRON', 'NOUN', 'PUNCT']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentimentanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modules & Variabelen klaarzetten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gebruik onderstaande code-blok om `lexicon` vanuit lexicondict.json in te laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon inlezen. Deze bevindt zich in lexicondict.json.\n",
    "import json\n",
    "with open(\"lexicondict.json\", \"rb\") as file: \n",
    "    lexicon = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaande code-blok maakt ook `spacy` klaar voor gebruik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeren van spacy\n",
    "import spacy\n",
    "#inladen van Nederlandse bibliotheek\n",
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenslotte de review die we willen onderzoeken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Nieuw concept in Gent, maar dat kan volgens mij toch beter. De meeste cornflakes waren gewoon de basic soorten. Ook wat duur voor de hoeveelheid die je krijgt, vooral met de toppings zijn ze zuinig. En als je ontbijt aanbiedt, geef de mensen dan toch ook wat meer keuze voor hun koffie.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentimentanalyse uitvoeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gebruik alle kennis die je in deze en de vorige notebook hebt opgedaan. Voer een sentimentanalyse op de `review`, gebruik makend van **spacy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../_afbeeldingen/ML_sentiment.png\" alt=\"Banner\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px; width:80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voer hier de sentimentanalyse met spacy uit.\n",
    "sentiment_score = 0\n",
    "\n",
    "# TODO: VUL VERDER AAN\n",
    "\n",
    "# Print de bekomen sentiment score\n",
    "print(sentiment_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heb je de sentimentanalyse af? Dan kan je onderstaande code-cel gebruiken om een zin te printen bij de bekomen `sentiment_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eindbeslissing voor deze review\n",
    "if sentiment_score > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif sentiment_score == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif sentiment_score < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(f\"De sentimentscore van de review is: {sentiment_score}\")\n",
    "print(\"Het sentiment van de review is \" + sentiment + \".\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oefen mee 3.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bekijk volgende reviews. \n",
    "\n",
    "-  Kies er twee uit en lees deze.\n",
    "\n",
    "-  Welk sentiment koppel jij zelf aan deze reviews? <div style=\"background-color:#008000\">\n",
    "    - review 1:\n",
    "    - review 2:\n",
    "</div>\n",
    "\n",
    "-  Voer een sentimentanalyse uit met de ontwikkelde code uit oefen mee 3.1. \n",
    "\n",
    "-  Vergelijk de sentimentscore met je eigen verwachtingen. Komen ze overeen?  <div style=\"background-color:#008000\">\n",
    "    - Antwoord:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sunrisewater is net wat België nodig heeft. Met het obesitasprobleem dat toch wel aan een opmars bezig is, kunnen we alle initiatieven gebruiken om de jeugd weer gewoon water te laten drinken in plaats van die Amerikaanse bucht! Het smaakt geweldig en wat nog beter is, is dat je het gewoon op elke straathoek kan vinden! Echt geweldig! Vooral de pink and yellow is ten zeerste aan te raden.\n",
    "\n",
    ">  Salé & Sucré staat bekend voor zijn super lekkere en originele cocktails, helaas was er geen alcoholvrije variant te verkrijgen. Onze BOB van dienst moest het dan maar bij frisdrank houden.\n",
    "\n",
    ">  Het was superleuk om eens te mogen proeven van de Filipijnse keuken. De gerechten zaten goed in elkaar, de porties waren zeker groot genoeg en de smaken zaten helemaal goed. Voor herhaling vatbaar!\n",
    "\n",
    ">  Gezellige sfeer, lekkere koffie en een mooi interieur. De combinatie van een studiebar en een babbelbar is een geniaal idee! Studeren met een lekker bakkie koffie, een overheerlijk hapje en samen met andere studenten, werkt enorm motiverend. Het interieur is enorm rustgevend met weinig afleiding, waardoor ik nog nooit zoveel heb kunnen doen!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B8000\">\n",
    "Proficiat, je hebt jouw regelgebaseerd (AI) systeem gecombineerd met een lerend (ML) systeem. Hierdoor is de sentimentanalyse volledig geautomatiseerd!\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "  <a href=\"https://www.aiopschool.be/chatbot/\"> \n",
    "        <img src=\"../_afbeeldingen/bannerugentdwengo.png\" alt=\"Dwengo\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 30px; width:20%\"/>\n",
    "    </a>\n",
    "\n",
    "Deze Notebook is gebaseerd op: Notebook Chatbot, zie <a href=\"http://www.aiopschool.be\">AI Op School</a>, van S. Pletinck , F. wyffels & N. Gesquière is in licentie gegeven volgens een <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Naamsvermelding-NietCommercieel-GelijkDelen 4.0 Internationaal-licentie</a>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
